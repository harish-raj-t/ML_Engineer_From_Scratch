{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Tutorial: From Theory to Implementation\n",
    "\n",
    "Welcome to this hands-on tutorial on Linear Regression! This notebook will guide you through implementing three different approaches to Linear Regression from scratch:\n",
    "\n",
    "1. **Batch Gradient Descent (BGD)**\n",
    "2. **Mini-batch Stochastic Gradient Descent (Mini-batch SGD)**\n",
    "3. **Normal Equation (Closed-form solution)**\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "- Understand how to vectorize data for machine learning\n",
    "- Implement forward pass, loss computation, and gradient descent\n",
    "- Understand the differences between optimization methods\n",
    "- Compare your implementations with scikit-learn\n",
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "Each section contains:\n",
    "- **Questions** that challenge you to think and implement\n",
    "- **Starter code** with TODO comments\n",
    "- **Hints** (expandable sections) to guide you\n",
    "- **Solutions** (expandable sections) for reference\n",
    "\n",
    "Try to solve each question before looking at hints or solutions!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preparation & Vectorization\n",
    "\n",
    "Understanding how data is organized is crucial for implementing machine learning algorithms efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: How to load and inspect the diabetes dataset?\n",
    "\n",
    "**Challenge**: Load the diabetes dataset from sklearn and understand its structure.\n",
    "\n",
    "**Think about**:\n",
    "- What information does the dataset contain?\n",
    "- How many features and samples are there?\n",
    "- What are we trying to predict?\n",
    "\n",
    "**Expected output**: Display basic information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "import pandas as pd\n",
    "\n",
    "# TODO: Load the diabetes dataset\n",
    "# Hint: Use load_diabetes with as_frame=True to get a DataFrame\n",
    "\n",
    "# TODO: Extract the features (X) and target (y)\n",
    "\n",
    "# TODO: Print the shape of X and y\n",
    "\n",
    "# TODO: Display the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Hint 1</summary>\n",
    "\n",
    "Use `load_diabetes(as_frame=True)` to get the data as a pandas DataFrame. The result will have a `.frame` attribute.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint 2</summary>\n",
    "\n",
    "```python\n",
    "data = load_diabetes(as_frame=True)\n",
    "df = data.frame\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>‚úÖ Solution</summary>\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "import pandas as pd\n",
    "\n",
    "# Load the diabetes dataset\n",
    "data = load_diabetes(as_frame=True)\n",
    "df = data.frame\n",
    "\n",
    "# Extract features and target\n",
    "X = df.drop(columns=['target']).values\n",
    "y = df['target'].values\n",
    "\n",
    "# Print shapes\n",
    "print(f\"X shape: {X.shape}\")  # (442, 10)\n",
    "print(f\"y shape: {y.shape}\")  # (442,)\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows of features:\")\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "**Explanation**: The diabetes dataset has 442 samples with 10 features each. We're predicting disease progression (target).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: How to organize data as matrices? (Rows vs Columns)\n",
    "\n",
    "**Challenge**: Verify that rows represent data points and columns represent features.\n",
    "\n",
    "**Think about**:\n",
    "- What does each row represent?\n",
    "- What does each column represent?\n",
    "- Why is this convention important for vectorization?\n",
    "\n",
    "**Expected output**: Print the shape and verify the convention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print X.shape and explain what each dimension means\n",
    "\n",
    "# TODO: Access the first data point (first row)\n",
    "\n",
    "# TODO: Access the first feature across all data points (first column)\n",
    "\n",
    "# TODO: Explain why this convention (rows=samples, columns=features) is standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Hint 1</summary>\n",
    "\n",
    "In machine learning:\n",
    "- Shape: (n_samples, n_features) or (m, n)\n",
    "- Each row: One complete data point with all its features\n",
    "- Each column: One feature across all data points\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>‚úÖ Solution</summary>\n",
    "\n",
    "```python\n",
    "print(f\"X shape: {X.shape}\")  # (442, 10)\n",
    "print(f\"Number of samples (m): {X.shape[0]}\")\n",
    "print(f\"Number of features (n): {X.shape[1]}\")\n",
    "\n",
    "# First data point (all features for sample 0)\n",
    "print(f\"\\nFirst data point: {X[0]}\")\n",
    "\n",
    "# First feature (across all samples)\n",
    "print(f\"\\nFirst feature across all samples: {X[:, 0]}\")\n",
    "\n",
    "# Convention explanation\n",
    "print(\"\"\"\n",
    "Convention: Rows = Datapoints, Columns = Features\n",
    "- This allows matrix multiplication: X @ weights\n",
    "- Shape (442, 10) @ (10, 1) = (442, 1) predictions\n",
    "- Each row represents a complete observation\n",
    "- Each column represents a variable/feature\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: How to split data into training and testing sets?\n",
    "\n",
    "**Challenge**: Split the data into training (80%) and testing (20%) sets.\n",
    "\n",
    "**Think about**:\n",
    "- Why do we need separate train and test sets?\n",
    "- What does `random_state` do?\n",
    "\n",
    "**Expected output**: X_train, X_test, y_train, y_test with appropriate shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TODO: Split data into train (80%) and test (20%) sets\n",
    "# Use random_state=42 for reproducibility\n",
    "\n",
    "# TODO: Print shapes of all four arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>‚úÖ Solution</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")  # (353, 10)\n",
    "print(f\"X_test shape: {X_test.shape}\")    # (89, 10)\n",
    "print(f\"y_train shape: {y_train.shape}\")  # (353,)\n",
    "print(f\"y_test shape: {y_test.shape}\")    # (89,)\n",
    "```\n",
    "\n",
    "**Why split?** To evaluate model performance on unseen data and prevent overfitting.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Batch Gradient Descent Implementation\n",
    "\n",
    "Now let's implement Linear Regression using Batch Gradient Descent from scratch!\n",
    "\n",
    "**Key Equations**:\n",
    "- Forward pass: $\\hat{y} = X \\cdot W + b$\n",
    "- Loss: $J = \\frac{1}{2m} \\sum (\\hat{y} - y)^2$\n",
    "- Gradients: $dW = \\frac{1}{m} X^T \\cdot error$, $db = \\frac{1}{m} \\sum error$\n",
    "- Update: $W = W - \\alpha \\cdot dW$, $b = b - \\alpha \\cdot db$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: How to initialize weights and bias?\n",
    "\n",
    "**Challenge**: Create a LinearRegressionSGD class and initialize weights and bias.\n",
    "\n",
    "**Think about**:\n",
    "- What should be the shape of weights?\n",
    "- What value should we initialize them to?\n",
    "- Why is bias a scalar?\n",
    "\n",
    "**Expected output**: Initialized weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionSGD:\n",
    "    def __init__(self, alpha=0.1, iterations=1000):\n",
    "        # TODO: Store hyperparameters\n",
    "        # TODO: Initialize weights and bias to None (will set in fit method)\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # TODO: Get number of instances and features from X.shape\n",
    "        \n",
    "        # TODO: Initialize weights as zeros with shape (n_features,)\n",
    "        \n",
    "        # TODO: Initialize bias as 0\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Hint 1</summary>\n",
    "\n",
    "If X has shape (m, n), weights should have shape (n,) to allow matrix multiplication X @ weights.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>‚úÖ Solution</summary>\n",
    "\n",
    "```python\n",
    "class LinearRegressionSGD:\n",
    "    def __init__(self, alpha=0.1, iterations=1000):\n",
    "        self.alpha = alpha\n",
    "        self.iterations = iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_instances, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "```\n",
    "\n",
    "**Explanation**: \n",
    "- Weights have shape (n_features,) - one weight per feature\n",
    "- Bias is a single scalar value\n",
    "- Initialize to zeros (safe for linear regression, though random init is used in deep learning)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: How to implement the forward pass?\n",
    "\n",
    "**Challenge**: Implement the prediction formula: $\\hat{y} = X \\cdot W + b$\n",
    "\n",
    "**Think about**:\n",
    "- What operation combines X and weights?\n",
    "- How is bias added to all predictions?\n",
    "- What should be the shape of pred_y?\n",
    "\n",
    "**Expected output**: Predictions for all training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this inside the fit method, after initialization\n",
    "\n",
    "# for i in range(self.iterations):\n",
    "#     # TODO: Forward pass - compute predictions\n",
    "#     # pred_y = ?\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Hint 1</summary>\n",
    "\n",
    "Use `np.dot(X, self.weights)` or `X @ self.weights` for matrix multiplication.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint 2</summary>\n",
    "\n",
    "```python\n",
    "pred_y = np.dot(X, self.weights) + self.bias\n",
    "```\n",
    "\n",
    "Shape: (m, n) @ (n,) + scalar = (m,)\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>‚úÖ Solution</summary>\n",
    "\n",
    "```python\n",
    "for i in range(self.iterations):\n",
    "    # Forward pass\n",
    "    pred_y = np.dot(X, self.weights) + self.bias\n",
    "```\n",
    "\n",
    "**Explanation**: This computes predictions for ALL samples simultaneously (vectorized operation).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: How to compute the error vector?\n",
    "\n",
    "**Challenge**: Calculate the difference between predictions and actual values.\n",
    "\n",
    "**Think about**:\n",
    "- What is error?\n",
    "- What should be the shape of error?\n",
    "- Why is this called \"residual\"?\n",
    "\n",
    "**Expected output**: Error vector with shape (m,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add after forward pass\n",
    "# TODO: Compute error\n",
    "# error = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>‚úÖ Solution</summary>\n",
    "\n",
    "```python\n",
    "error = pred_y - y\n",
    "```\n",
    "\n",
    "**Explanation**: Error (residual) is how much our prediction differs from the true value. Positive error means we over-predicted, negative means under-predicted.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: How to compute the loss (cost function)?\n",
    "\n",
    "**Challenge**: Calculate Mean Squared Error (MSE).\n",
    "\n",
    "**Think about**:\n",
    "- Why square the error?\n",
    "- Why take the mean?\n",
    "- How to do this efficiently with numpy?\n",
    "\n",
    "**Expected output**: A single scalar loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute loss (MSE)\n",
    "# loss = ?\n",
    "\n",
    "# TODO: Print loss every 1000 iterations\n",
    "# if i % 1000 == 0:\n",
    "#     print(f\"loss at {i} iteration is {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Hint 1</summary>\n",
    "\n",
    "MSE = mean of squared errors. Use `np.mean(error**2)`.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>‚úÖ Solution</summary>\n",
    "\n",
    "```python\n",
    "loss = np.mean(error**2)\n",
    "\n",
    "if i % 1000 == 0:\n",
    "    print(f\"loss at {i} iteration is {loss}\")\n",
    "```\n",
    "\n",
    "**Why MSE?**\n",
    "- Squaring makes all errors positive\n",
    "- Larger errors get penalized more (quadratic penalty)\n",
    "- Differentiable for gradient computation\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8: How to compute gradients?\n",
    "\n",
    "**Challenge**: Implement gradient computation for weights and bias.\n",
    "\n",
    "**Think about**:\n",
    "- Why do we need $X^T$ (transpose)?\n",
    "- Why divide by m (number of samples)?\n",
    "- What's the relationship between error and gradients?\n",
    "\n",
    "**Expected output**: dw and db with correct shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute gradient for weights\n",
    "# dw = ?\n",
    "\n",
    "# TODO: Compute gradient for bias\n",
    "# db = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Hint 1</summary>\n",
    "\n",
    "Formulas:\n",
    "- $dW = \\frac{1}{m} X^T \\cdot error$\n",
    "- $db = \\frac{1}{m} \\sum error$\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint 2</summary>\n",
    "\n",
    "```python\n",
    "dw = np.dot(X.T, error) / n_instances\n",
    "db = np.sum(error) / n_instances\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>‚úÖ Solution</summary>\n",
    "\n",
    "```python\n",
    "# Gradients computation\n",
    "dw = np.dot(X.T, error) / n_instances\n",
    "db = np.sum(error) / n_instances\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "- `X.T @ error` computes how much each feature contributes to total error\n",
    "- Shape: (n, m) @ (m,) = (n,) - one gradient per weight\n",
    "- Division by m gives average gradient across all samples\n",
    "- db is simpler: just average of all errors\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9: How to update weights using learning rate?\n",
    "\n",
    "**Challenge**: Implement the gradient descent update rule.\n",
    "\n",
    "**Think about**:\n",
    "- Why subtract (not add) the gradient?\n",
    "- What does learning rate control?\n",
    "- What happens if alpha is too large or too small?\n",
    "\n",
    "**Expected output**: Updated weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update weights\n",
    "# self.weights = ?\n",
    "\n",
    "# TODO: Update bias\n",
    "# self.bias = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Hint 1</summary>\n",
    "\n",
    "Gradient descent moves in the OPPOSITE direction of the gradient (steepest descent).\n",
    "\n",
    "Update rule: $\\theta = \\theta - \\alpha \\cdot d\\theta$\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>‚úÖ Solution</summary>\n",
    "\n",
    "```python\n",
    "# Weights update\n",
    "self.weights = self.weights - self.alpha * dw\n",
    "self.bias = self.bias - self.alpha * db\n",
    "```\n",
    "\n",
    "**Key Points**:\n",
    "- Negative gradient direction leads to minimum\n",
    "- Learning rate (alpha) controls step size\n",
    "- Too large alpha ‚Üí overshooting, divergence\n",
    "- Too small alpha ‚Üí slow convergence\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10: Complete the predict method\n",
    "\n",
    "**Challenge**: Implement prediction using learned weights.\n",
    "\n",
    "**Think about**:\n",
    "- Is this the same as forward pass?\n",
    "- Can we use this for test data?\n",
    "\n",
    "**Expected output**: Predictions for any input X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionSGD:\n",
    "    # ... (previous methods)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # TODO: Return predictions using learned weights and bias\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>‚úÖ Solution</summary>\n",
    "\n",
    "```python\n",
    "def predict(self, X):\n",
    "    return np.dot(X, self.weights) + self.bias\n",
    "```\n",
    "\n",
    "**Complete Class:**\n",
    "\n",
    "```python\n",
    "class LinearRegressionSGD:\n",
    "    def __init__(self, alpha=0.1, iterations=1000):\n",
    "        self.alpha = alpha\n",
    "        self.iterations = iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_instances, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        for i in range(self.iterations):\n",
    "            # Forward pass\n",
    "            pred_y = np.dot(X, self.weights) + self.bias\n",
    "            \n",
    "            # Error\n",
    "            error = pred_y - y\n",
    "            loss = np.mean(error**2)\n",
    "            \n",
    "            if i % 1000 == 0:\n",
    "                print(f\"loss at {i} iteration is {loss}\")\n",
    "            \n",
    "            # Gradients computation\n",
    "            dw = np.dot(X.T, error) / n_instances\n",
    "            db = np.sum(error) / n_instances\n",
    "            \n",
    "            # Weights update\n",
    "            self.weights = self.weights - self.alpha * dw\n",
    "            self.bias = self.bias - self.alpha * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11: Test your Batch Gradient Descent implementation\n",
    "\n",
    "**Challenge**: Train the model and make predictions.\n",
    "\n",
    "**Expected output**: Loss decreasing over iterations, predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create an instance of LinearRegressionSGD with alpha=0.1 and iterations=10000\n",
    "\n",
    "# TODO: Fit the model on training data\n",
    "\n",
    "# TODO: Make predictions on test data\n",
    "\n",
    "# TODO: Print first 10 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>‚úÖ Solution</summary>\n",
    "\n",
    "```python\n",
    "model = LinearRegressionSGD(alpha=0.1, iterations=10000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"\\nFirst 10 predictions: {y_pred[:10]}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Mini-batch Stochastic Gradient Descent\n",
    "\n",
    "Now let's implement a more efficient version using mini-batches!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12: Why shuffle data? Implement shuffling\n",
    "\n",
    "**Challenge**: Shuffle the training data before creating batches.\n",
    "\n",
    "**Think about**:\n",
    "- Why shuffle before each epoch?\n",
    "- How to shuffle X and y together (keep pairs intact)?\n",
    "- What does `np.random.permutation` do?\n",
    "\n",
    "**Expected output**: Shuffled indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate shuffled indices using np.random.permutation\n",
    "# indices = ?\n",
    "\n",
    "# TODO: Use indices to shuffle X and y\n",
    "# X_shuffled = ?\n",
    "# y_shuffled = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Hint 1</summary>\n",
    "\n",
    "```python\n",
    "indices = np.random.permutation(n_instances)\n",
    "```\n",
    "\n",
    "This returns shuffled indices from 0 to n_instances-1.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>‚úÖ Solution</summary>\n",
    "\n",
    "```python\n",
    "# Shuffle the data\n",
    "indices = np.random.permutation(n_instances)\n",
    "X_shuffled = X[indices]\n",
    "y_shuffled = y[indices]\n",
    "```\n",
    "\n",
    "**Why shuffle?**\n",
    "- Prevents model from learning order-dependent patterns\n",
    "- Ensures each batch is representative of full dataset\n",
    "- Improves convergence in SGD/mini-batch\n",
    "\n",
    "Example: If n_instances=100, indices might be [47, 2, 89, 15, ...] (random order)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13: How to create batches?\n",
    "\n",
    "**Challenge**: Implement batch creation logic.\n",
    "\n",
    "**Think about**:\n",
    "- How to split data into chunks of batch_size?\n",
    "- What happens to the last batch if it's smaller?\n",
    "- How to loop through batches?\n",
    "\n",
    "**Expected output**: Batches of size 64 (or smaller for last batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Loop through batches using range with step=batch_size\n",
    "# for i in range(?, ?, ?):\n",
    "\n",
    "# TODO: Extract batch from shuffled data\n",
    "# X_batch = ?\n",
    "# y_batch = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Hint 1</summary>\n",
    "\n",
    "Use `range(0, n_instances, batch_size)` to get starting indices: 0, 64, 128, 192, ...\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint 2</summary>\n",
    "\n",
    "```python\n",
    "for i in range(0, n_instances, self.batch_size):\n",
    "    X_batch = X_shuffled[i:i+self.batch_size]\n",
    "    y_batch = y_shuffled[i:i+self.batch_size]\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>‚úÖ Solution</summary>\n",
    "\n",
    "```python\n",
    "for i in range(0, n_instances, self.batch_size):\n",
    "    X_batch = X_shuffled[i:i+self.batch_size]\n",
    "    y_batch = y_shuffled[i:i+self.batch_size]\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "- If n_instances=353 and batch_size=64:\n",
    "  - Batch 1: indices 0:64 (64 samples)\n",
    "  - Batch 2: indices 64:128 (64 samples)\n",
    "  - Batch 3: indices 128:192 (64 samples)\n",
    "  - ...\n",
    "  - Last batch: indices 320:353 (33 samples) ‚Üê smaller!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14: Implement Mini-batch SGD\n",
    "\n",
    "**Challenge**: Complete the LinearRegressionMiniSGD class.\n",
    "\n",
    "**Think about**:\n",
    "- How does this differ from Batch GD?\n",
    "- Why use `len(X_batch)` instead of `n_instances`?\n",
    "- What's the tradeoff with batch_size?\n",
    "\n",
    "**Expected output**: Complete working Mini-batch SGD implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionMiniSGD:\n",
    "    def __init__(self, alpha=0.1, iterations=1000, batch_size=64):\n",
    "        # TODO: Initialize hyperparameters\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # TODO: Initialize weights and bias\n",
    "        \n",
    "        # TODO: Loop for iterations (epochs)\n",
    "        # for _ in range(self.iterations):\n",
    "        \n",
    "            # TODO: Shuffle data\n",
    "            \n",
    "            # TODO: Loop through batches\n",
    "            # for i in range(0, n_instances, self.batch_size):\n",
    "            \n",
    "                # TODO: Get batch\n",
    "                \n",
    "                # TODO: Forward pass on batch\n",
    "                \n",
    "                # TODO: Compute error on batch\n",
    "                \n",
    "                # TODO: Compute gradients using len(X_batch)\n",
    "                \n",
    "                # TODO: Update weights\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # TODO: Return predictions\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>‚úÖ Solution</summary>\n",
    "\n",
    "```python\n",
    "class LinearRegressionMiniSGD:\n",
    "    def __init__(self, alpha=0.1, iterations=1000, batch_size=64):\n",
    "        self.alpha = alpha\n",
    "        self.iterations = iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_instances, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Mini batch SGD\n",
    "        for _ in range(self.iterations):\n",
    "            # Shuffle the data\n",
    "            indices = np.random.permutation(n_instances)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            \n",
    "            for i in range(0, n_instances, self.batch_size):\n",
    "                X_batch = X_shuffled[i:i+self.batch_size]\n",
    "                y_batch = y_shuffled[i:i+self.batch_size]\n",
    "                \n",
    "                # Forward pass\n",
    "                pred_y = np.dot(X_batch, self.weights) + self.bias\n",
    "                error = pred_y - y_batch\n",
    "                \n",
    "                # Gradients (note: using len(X_batch), not n_instances)\n",
    "                dw = np.dot(X_batch.T, error) / len(X_batch)\n",
    "                db = np.sum(error) / len(X_batch)\n",
    "                \n",
    "                # Gradient update\n",
    "                self.weights = self.weights - self.alpha * dw\n",
    "                self.bias = self.bias - self.alpha * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "```\n",
    "\n",
    "**Key Differences from Batch GD**:\n",
    "- Updates happen multiple times per epoch (once per batch)\n",
    "- Uses `len(X_batch)` for averaging (handles variable-size last batch)\n",
    "- Faster convergence with less memory\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 15: Test Mini-batch SGD\n",
    "\n",
    "**Challenge**: Train and compare with Batch GD.\n",
    "\n",
    "**Expected output**: Predictions from Mini-batch SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and train LinearRegressionMiniSGD\n",
    "\n",
    "# TODO: Make predictions\n",
    "\n",
    "# TODO: Print first 10 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>‚úÖ Solution</summary>\n",
    "\n",
    "```python\n",
    "model_mini = LinearRegressionMiniSGD(alpha=0.1, iterations=10000, batch_size=64)\n",
    "model_mini.fit(X_train, y_train)\n",
    "y_pred_mini = model_mini.predict(X_test)\n",
    "print(f\"First 10 predictions: {y_pred_mini[:10]}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Normal Equation (Closed-form Solution)\n",
    "\n",
    "Instead of iterative optimization, we can solve directly using linear algebra!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 16: Understand the Normal Equation\n",
    "\n",
    "**Challenge**: Implement the closed-form solution: $\\theta = (X^T X)^{-1} X^T y$\n",
    "\n",
    "**Think about**:\n",
    "- Why does this work?\n",
    "- What if we want to include an intercept?\n",
    "- What's the difference between centering and adding a bias column?\n",
    "\n",
    "**Expected output**: Optimal weights computed directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionNormalEqn:\n",
    "    def __init__(self, fit_intercept=True):\n",
    "        # TODO: Store parameters\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            # TODO: Center X (subtract mean)\n",
    "            # Xc = ?\n",
    "            \n",
    "            # TODO: Center y\n",
    "            # yc = ?\n",
    "            \n",
    "            # TODO: Use np.linalg.lstsq to solve for parameters\n",
    "            # parameters, *_ = np.linalg.lstsq(?, ?, rcond=None)\n",
    "            \n",
    "            # TODO: Store coefficients\n",
    "            # self.coef = ?\n",
    "            \n",
    "            # TODO: Calculate intercept from means\n",
    "            # self.intercept = ?\n",
    "            pass\n",
    "        else:\n",
    "            # TODO: Solve without intercept\n",
    "            pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # TODO: Return predictions\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Hint 1</summary>\n",
    "\n",
    "Centering data means subtracting the mean:\n",
    "```python\n",
    "Xc = X - X.mean(axis=0)\n",
    "yc = y - y.mean()\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint 2</summary>\n",
    "\n",
    "`np.linalg.lstsq` solves the least squares problem. After solving with centered data, the intercept can be recovered:\n",
    "```python\n",
    "intercept = y.mean() - X.mean(axis=0) @ coef\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>‚úÖ Solution</summary>\n",
    "\n",
    "```python\n",
    "class LinearRegressionNormalEqn:\n",
    "    def __init__(self, fit_intercept=True):\n",
    "        self.coef = None\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.intercept = 0.0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            # Center the data\n",
    "            Xc = X - X.mean(axis=0)\n",
    "            yc = y - y.mean()\n",
    "            \n",
    "            # Solve using least squares\n",
    "            parameters, *_ = np.linalg.lstsq(Xc, yc, rcond=None)\n",
    "            self.coef = parameters\n",
    "            \n",
    "            # Calculate intercept\n",
    "            self.intercept = y.mean() - X.mean(axis=0) @ self.coef\n",
    "        else:\n",
    "            parameters, *_ = np.linalg.lstsq(X, y, rcond=None)\n",
    "            self.coef = parameters\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X @ self.coef + self.intercept\n",
    "```\n",
    "\n",
    "**Why center data?**\n",
    "- Centering allows fitting intercept without adding a column of 1's\n",
    "- Mathematically equivalent to Normal Equation with augmented X\n",
    "- More numerically stable\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 17: Test Normal Equation\n",
    "\n",
    "**Challenge**: Train using Normal Equation and compare with GD methods.\n",
    "\n",
    "**Expected output**: Predictions from Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and train LinearRegressionNormalEqn\n",
    "\n",
    "# TODO: Make predictions\n",
    "\n",
    "# TODO: Print first 10 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>‚úÖ Solution</summary>\n",
    "\n",
    "```python\n",
    "model_normal = LinearRegressionNormalEqn()\n",
    "model_normal.fit(X_train, y_train)\n",
    "y_pred_normal = model_normal.predict(X_test)\n",
    "print(f\"First 10 predictions: {y_pred_normal[:10]}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Comparison & Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 18: Compare with sklearn\n",
    "\n",
    "**Challenge**: Train sklearn's LinearRegression and compare predictions.\n",
    "\n",
    "**Think about**:\n",
    "- How close are your implementations to sklearn?\n",
    "- Which method does sklearn use internally?\n",
    "\n",
    "**Expected output**: sklearn predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# TODO: Create and train sklearn model\n",
    "\n",
    "# TODO: Make predictions\n",
    "\n",
    "# TODO: Print first 10 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>‚úÖ Solution</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model_sklearn = LinearRegression()\n",
    "model_sklearn.fit(X_train, y_train)\n",
    "y_pred_sklearn = model_sklearn.predict(X_test)\n",
    "print(f\"First 10 predictions: {y_pred_sklearn[:10]}\")\n",
    "```\n",
    "\n",
    "**Note**: sklearn uses a variant of Normal Equation (SVD-based) for numerical stability.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 19: Calculate and compare metrics\n",
    "\n",
    "**Challenge**: Compute Mean Squared Error (MSE) for all methods.\n",
    "\n",
    "**Think about**:\n",
    "- Which method gives the best results?\n",
    "- Are the results similar?\n",
    "- Why might they differ slightly?\n",
    "\n",
    "**Expected output**: MSE comparison table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# TODO: Compute MSE for all methods\n",
    "# mse_bgd = ?\n",
    "# mse_mini = ?\n",
    "# mse_normal = ?\n",
    "# mse_sklearn = ?\n",
    "\n",
    "# TODO: Print comparison table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>‚úÖ Solution</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming you have predictions from all models\n",
    "mse_bgd = mean_squared_error(y_test, y_pred)\n",
    "mse_mini = mean_squared_error(y_test, y_pred_mini)\n",
    "mse_normal = mean_squared_error(y_test, y_pred_normal)\n",
    "mse_sklearn = mean_squared_error(y_test, y_pred_sklearn)\n",
    "\n",
    "print(\"\\n=== Model Comparison ===\")\n",
    "print(f\"Batch Gradient Descent MSE: {mse_bgd:.2f}\")\n",
    "print(f\"Mini-batch SGD MSE: {mse_mini:.2f}\")\n",
    "print(f\"Normal Equation MSE: {mse_normal:.2f}\")\n",
    "print(f\"sklearn LinearRegression MSE: {mse_sklearn:.2f}\")\n",
    "```\n",
    "\n",
    "**Expected Result**: All methods should give very similar MSE (within small rounding errors).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 20: Final Analysis Questions\n",
    "\n",
    "**Reflect on what you've learned**:\n",
    "\n",
    "1. **When would you use each method?**\n",
    "   - Batch GD: ?\n",
    "   - Mini-batch SGD: ?\n",
    "   - Normal Equation: ?\n",
    "\n",
    "2. **What are the computational complexities?**\n",
    "   - Gradient Descent: ?\n",
    "   - Normal Equation: ?\n",
    "\n",
    "3. **What happens if X^T X is not invertible?**\n",
    "\n",
    "4. **How does batch size affect training?**\n",
    "\n",
    "5. **What would you change if you had millions of samples?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>‚úÖ Answers</summary>\n",
    "\n",
    "**1. When to use each method:**\n",
    "- **Batch GD**: Small to medium datasets, when you want smooth convergence\n",
    "- **Mini-batch SGD**: Large datasets, online learning, when memory is limited\n",
    "- **Normal Equation**: Small number of features (n < 10,000), need exact solution\n",
    "\n",
    "**2. Computational complexity:**\n",
    "- **Gradient Descent**: O(k¬∑m¬∑n) where k = iterations, m = samples, n = features\n",
    "- **Normal Equation**: O(n¬≥) due to matrix inversion + O(n¬≤¬∑m) for X^T X\n",
    "\n",
    "**3. Non-invertible X^T X:**\n",
    "- Happens when features are linearly dependent (multicollinearity)\n",
    "- Solutions:\n",
    "  - Remove redundant features\n",
    "  - Use regularization (Ridge: adds ŒªI to make it invertible)\n",
    "  - Use pseudo-inverse\n",
    "  - Use gradient descent instead\n",
    "\n",
    "**4. Batch size effects:**\n",
    "- **Small batches** (1-32): Noisy updates, faster per iteration, may escape local minima\n",
    "- **Large batches** (256+): Stable updates, better hardware utilization, but slower per epoch\n",
    "- **Sweet spot**: Usually 32-128 for good balance\n",
    "\n",
    "**5. For millions of samples:**\n",
    "- Use Mini-batch SGD (not Batch GD or Normal Equation)\n",
    "- Consider smaller batch sizes for memory\n",
    "- Use momentum-based optimizers (Adam, RMSprop)\n",
    "- Consider distributed training\n",
    "- May need early stopping to avoid overfitting\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Congratulations! üéâ\n",
    "\n",
    "You've successfully implemented Linear Regression from scratch using three different approaches:\n",
    "\n",
    "1. ‚úÖ Batch Gradient Descent\n",
    "2. ‚úÖ Mini-batch Stochastic Gradient Descent\n",
    "3. ‚úÖ Normal Equation (Closed-form)\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Vectorization** is crucial for efficient implementation\n",
    "- **Gradient descent** works by iteratively moving towards the minimum\n",
    "- **Mini-batch SGD** balances speed and stability\n",
    "- **Normal Equation** gives exact solution but doesn't scale to large features\n",
    "- All methods converge to similar solutions for linear regression\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try with different datasets\n",
    "- Experiment with different learning rates and batch sizes\n",
    "- Add regularization (Ridge, Lasso)\n",
    "- Implement feature scaling/normalization\n",
    "- Visualize the loss curves\n",
    "\n",
    "For complete reference implementations, check out the **Solution Reference** notebook!\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

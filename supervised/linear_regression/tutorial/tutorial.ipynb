{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Tutorial: From Theory to Implementation\n",
    "\n",
    "Welcome to this hands-on tutorial on Linear Regression! This notebook will guide you through implementing three different approaches to Linear Regression from scratch:\n",
    "\n",
    "1. **Batch Gradient Descent (BGD)**\n",
    "2. **Mini-batch Stochastic Gradient Descent (Mini-batch SGD)**\n",
    "3. **Normal Equation (Closed-form solution)**\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "- Understand how to vectorize data for machine learning\n",
    "- Understand forward pass, loss computation, gradients, and weight updates\n",
    "- Implement complete gradient descent algorithms\n",
    "- Understand the differences between optimization methods\n",
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "Each section contains:\n",
    "- **Questions** that challenge you to think and implement\n",
    "- **Starter code** with TODO comments\n",
    "- **Hints** (expandable sections) to guide you\n",
    "\n",
    "Try to solve each question before looking at hints!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Data Preparation & Vectorization\n",
    "\n",
    "Understanding how data is organized is crucial for implementing machine learning algorithms efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Dataset\n",
    "\n",
    "Here's a small dataset about house prices with 5 samples and 3 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataset: House prices\n",
    "data = {\n",
    "    'size_sqft': [1400, 1600, 1700, 1875, 1100],\n",
    "    'bedrooms': [3, 3, 2, 4, 2],\n",
    "    'age_years': [20, 15, 10, 5, 25],\n",
    "    'price': [245000, 312000, 279000, 308000, 199000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: How to extract features (X) and target (y) from the dataset?\n",
    "\n",
    "**Challenge**: Extract the feature matrix X and target vector y from the given dataset.\n",
    "\n",
    "**Think about**:\n",
    "- What are we trying to predict? (This becomes y)\n",
    "- What information do we use to make predictions? (This becomes X)\n",
    "- What should be the shape of X? (rows vs columns)\n",
    "- What should be the shape of y?\n",
    "- What does each row represent?\n",
    "- What does each column represent?\n",
    "\n",
    "**Expected output**: \n",
    "- X with shape (5, 3) - 5 samples, 3 features\n",
    "- y with shape (5,) - 5 target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'size_sqft': [1400, 1600, 1700, 1875, 1100],\n",
    "    'bedrooms': [3, 3, 2, 4, 2],\n",
    "    'age_years': [20, 15, 10, 5, 25],\n",
    "    'price': [245000, 312000, 279000, 308000, 199000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# TODO: Extract features (X) - all columns except 'price'\n",
    "# X = ?\n",
    "\n",
    "# TODO: Extract target (y) - the 'price' column\n",
    "# y = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Hint 1</summary>\n",
    "\n",
    "In machine learning:\n",
    "- **Target (y)**: What we want to predict (in this case: 'price')\n",
    "- **Features (X)**: Information we use to make predictions (in this case: 'size_sqft', 'bedrooms', 'age_years')\n",
    "\n",
    "Use `df.drop(columns=['price'])` to get features and `df['price']` to get target.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>ðŸ’¡ Hint 2</summary>\n",
    "\n",
    "Remember to convert to numpy arrays using `.values`:\n",
    "```python\n",
    "X = df.drop(columns=['price']).values\n",
    "y = df['price'].values\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Understanding Linear Regression Components\n",
    "\n",
    "Before implementing the complete algorithm, let's understand each component individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1: Forward Pass - Making Predictions\n",
    "\n",
    "The forward pass calculates predictions using the linear equation: $\\hat{y} = X \\cdot W + b$\n",
    "\n",
    "Where:\n",
    "- $X$ = Feature matrix (m Ã— n)\n",
    "- $W$ = Weight vector (n,)\n",
    "- $b$ = Bias (scalar)\n",
    "- $\\hat{y}$ = Predictions (m,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Given random weights and bias, compute predictions for our house prices.\n",
    "\n",
    "**Think about**:\n",
    "- What operation combines X and weights?\n",
    "- How is bias added?\n",
    "- What should be the shape of predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X and y from Part 1\n",
    "# X shape: (5, 3), y shape: (5,)\n",
    "\n",
    "# Initialize random weights and bias\n",
    "np.random.seed(42)\n",
    "weights = np.random.randn(3)  # 3 weights for 3 features\n",
    "bias = 0.0\n",
    "\n",
    "print(f\"Weights: {weights}\")\n",
    "print(f\"Bias: {bias}\")\n",
    "\n",
    "predictions = np.dot(X, weights) + bias\n",
    "\n",
    "# TODO: Compute predictions using forward pass\n",
    "# predictions = ?\n",
    "\n",
    "# TODO: Print predictions and their shape\n",
    "# print(f\"\\nPredictions: {predictions}\")\n",
    "# print(f\"Predictions shape: {predictions.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Hint</summary>\n",
    "\n",
    "Use matrix multiplication:\n",
    "```python\n",
    "predictions = np.dot(X, weights) + bias\n",
    "# or equivalently: predictions = X @ weights + bias\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2: Loss Function - Measuring Error\n",
    "\n",
    "The loss function measures how wrong our predictions are. We use Mean Squared Error (MSE):\n",
    "\n",
    "$$\\text{Loss} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i)^2$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{y}$ = Predictions\n",
    "- $y$ = Actual values\n",
    "- $m$ = Number of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Compute the MSE loss for our predictions.\n",
    "\n",
    "**Think about**:\n",
    "- What is the error (residual)?\n",
    "- Why square the error?\n",
    "- Why take the mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using predictions from previous step\n",
    "\n",
    "# TODO: Compute error (predictions - actual)\n",
    "# error = ?\n",
    "\n",
    "# TODO: Compute MSE loss\n",
    "# loss = ?\n",
    "\n",
    "# TODO: Print error and loss\n",
    "# print(f\"Error for each sample: {error}\")\n",
    "# print(f\"\\nMSE Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Hint</summary>\n",
    "\n",
    "```python\n",
    "error = predictions - y\n",
    "loss = np.mean(error**2)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.3: Computing Gradients\n",
    "\n",
    "Gradients tell us how to adjust weights to reduce loss. They are the partial derivatives of the loss with respect to each parameter:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W} = \\frac{1}{m} X^T \\cdot \\text{error}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{1}{m} \\sum \\text{error}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Compute gradients for weights and bias.\n",
    "\n",
    "**Think about**:\n",
    "- Why do we need $X^T$ (transpose)?\n",
    "- Why divide by m?\n",
    "- What should be the shape of gradients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using X, error from previous steps\n",
    "m = X.shape[0]  # Number of samples\n",
    "\n",
    "# TODO: Compute gradient for weights\n",
    "# dw = ?\n",
    "\n",
    "# TODO: Compute gradient for bias\n",
    "# db = ?\n",
    "\n",
    "# TODO: Print gradients and their shapes\n",
    "# print(f\"Weight gradients: {dw}\")\n",
    "# print(f\"Weight gradients shape: {dw.shape}\")\n",
    "# print(f\"\\nBias gradient: {db}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Hint</summary>\n",
    "\n",
    "```python\n",
    "dw = np.dot(X.T, error) / m\n",
    "db = np.sum(error) / m\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.4: Updating Weights\n",
    "\n",
    "Gradient descent updates parameters by moving in the opposite direction of the gradient:\n",
    "\n",
    "$$W_{\\text{new}} = W_{\\text{old}} - \\alpha \\cdot \\frac{\\partial L}{\\partial W}$$\n",
    "\n",
    "$$b_{\\text{new}} = b_{\\text{old}} - \\alpha \\cdot \\frac{\\partial L}{\\partial b}$$\n",
    "\n",
    "Where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Update weights and bias, then check if loss decreased.\n",
    "\n",
    "**Think about**:\n",
    "- Why subtract (not add) the gradient?\n",
    "- What does learning rate control?\n",
    "- Should loss go down after update?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using weights, bias, dw, db from previous steps\n",
    "alpha = 0.01  # Learning rate\n",
    "\n",
    "print(f\"Before update:\")\n",
    "print(f\"Weights: {weights}\")\n",
    "print(f\"Bias: {bias}\")\n",
    "print(f\"Loss: {loss}\")\n",
    "\n",
    "# TODO: Update weights\n",
    "# weights_new = ?\n",
    "\n",
    "# TODO: Update bias\n",
    "# bias_new = ?\n",
    "\n",
    "# TODO: Compute new predictions and loss\n",
    "# predictions_new = ?\n",
    "# error_new = ?\n",
    "# loss_new = ?\n",
    "\n",
    "# TODO: Print results\n",
    "# print(f\"\\nAfter update:\")\n",
    "# print(f\"Weights: {weights_new}\")\n",
    "# print(f\"Bias: {bias_new}\")\n",
    "# print(f\"Loss: {loss_new}\")\n",
    "# print(f\"\\nLoss decreased by: {loss - loss_new}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Hint</summary>\n",
    "\n",
    "```python\n",
    "weights_new = weights - alpha * dw\n",
    "bias_new = bias - alpha * db\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Batch Gradient Descent Implementation\n",
    "\n",
    "Now that you understand each component, let's implement the complete Batch Gradient Descent algorithm!\n",
    "\n",
    "**The Algorithm**:\n",
    "1. Initialize weights and bias\n",
    "2. For each iteration:\n",
    "   - Forward pass: compute predictions\n",
    "   - Compute error and loss\n",
    "   - Compute gradients\n",
    "   - Update weights\n",
    "3. Repeat until convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Implementation\n",
    "\n",
    "Implement the LinearRegressionSGD class with all components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionSGD:\n",
    "    def __init__(self, alpha=0.1, iterations=1000):\n",
    "        # TODO: Store hyperparameters and initialize weights/bias\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # TODO: Get dimensions and initialize weights\n",
    "        # TODO: Loop through iterations\n",
    "        # TODO: Forward pass, compute error, compute gradients, update weights\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # TODO: Return predictions\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Mini-batch Stochastic Gradient Descent\n",
    "\n",
    "Now let's implement a more efficient version using mini-batches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionMiniSGD:\n",
    "    def __init__(self, alpha=0.1, iterations=1000, batch_size=64):\n",
    "        # TODO: Initialize hyperparameters\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # TODO: Initialize weights and bias\n",
    "        # TODO: Loop for iterations (epochs)\n",
    "        # TODO: Shuffle data\n",
    "        # TODO: Loop through batches\n",
    "        # TODO: Forward pass on batch, compute gradients, update weights\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # TODO: Return predictions\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Normal Equation (Closed-form Solution)\n",
    "\n",
    "Instead of iterative optimization, we can solve directly using linear algebra!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionNormalEqn:\n",
    "    def __init__(self, fit_intercept=True):\n",
    "        # TODO: Store parameters\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # TODO: Center data if fit_intercept=True\n",
    "        # TODO: Use np.linalg.lstsq to solve\n",
    "        # TODO: Calculate intercept if needed\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # TODO: Return predictions\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Congratulations! ðŸŽ‰\n",
    "\n",
    "You've successfully learned and implemented Linear Regression from scratch!\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "1. âœ… **Data Vectorization**: How to organize data as matrices\n",
    "2. âœ… **Forward Pass**: Computing predictions using $\\hat{y} = XW + b$\n",
    "3. âœ… **Loss Function**: Measuring prediction error with MSE\n",
    "4. âœ… **Gradients**: Computing derivatives for optimization\n",
    "5. âœ… **Weight Updates**: Using gradient descent to minimize loss\n",
    "6. âœ… **Batch Gradient Descent**: Full implementation\n",
    "7. âœ… **Mini-batch SGD**: More efficient variant\n",
    "8. âœ… **Normal Equation**: Direct analytical solution\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Vectorization** is crucial for efficient implementation\n",
    "- **Gradient descent** iteratively moves towards the minimum loss\n",
    "- **Mini-batch SGD** balances speed and stability\n",
    "- **Normal Equation** gives exact solution but doesn't scale to large features\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Test your implementations on real datasets (e.g., sklearn's diabetes dataset)\n",
    "- Experiment with different learning rates and batch sizes\n",
    "- Add regularization (Ridge, Lasso)\n",
    "- Implement feature scaling/normalization\n",
    "- Visualize the loss curves and decision boundaries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
